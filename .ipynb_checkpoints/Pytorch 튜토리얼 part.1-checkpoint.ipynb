{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥러닝 개좆밥이 조금이라도 더 이해해보자고 쓰는 Pytorch 튜토리얼 1\n",
    "워낙에 아는게 없기 때문에 쓸데없어 보이는 별의별거 다 들어가는 튜토리얼\n",
    "\n",
    "### Pytorch는 무엇인가?\n",
    "pyTorch 공식 튜토리얼 https://pytorch.org/tutorials/ 에서는, PyTorch에 대해 다음과 같이 설명하고 있다.\n",
    "\n",
    "PyTorch는 파이썬 기반의 과학 컴퓨팅 패키지로 두 집단을 대상으로 합니다.\n",
    "   * NumPy를 대체하고 GPU의 힘을 사용\n",
    "   * 최대한의 유연성과 속도를 제공하는 딥러닝 연구 플래폼\n",
    "   \n",
    "...라고 설명되어 있다. 그러나 이 설명만으로는 기존에 Torch를 사용해보지 않았거나 딥러닝을 이제 막 공부하기 시작한 사람들에게는<br>\n",
    "충분하지 않은 설명이라고 생각되어 추가적으로 Torch에 대해 설명해보자면,\n",
    "\n",
    "## Torch\n",
    "Torch는 GPU를 사용하면서 객체지향적으로 모델을 만드는 Lua를 사용하는 딥러닝 라이브러리이다.<br>\n",
    "Torch는 다음과 같은 특징을 갖고 있다.\n",
    "   * Python보다 빠름. Just in time compilation(JIT 컴파일, 동적번역으로 프로그램을 실행하는 시점에 기계어로 번역하는 것) 때문에.\n",
    "   * Luarocks로 Python의 pip처럼 lua package를 설치   \n",
    "   * 페이스북, 딥마인드, 트위터 등 여러회사에서 사용\n",
    "   * Numpy처럼 다룰 수 있음\n",
    "   * 모듈안에 자신의 코드를 만들어서 사용가능\n",
    "   * 네트워크 구조를 짜는 것이 아닌, 텐서와 텐서를 연결하는 node를 만들어서 구현\n",
    "   \n",
    "과 같은 특징을 갖고 있다. 다음과 같은 특징들에서 나오는 Torch의 장점으로는 \n",
    "   * 결합시키기 쉬운 많은 모듈러 조각을 만들 수 있음\n",
    "   * 다른 딥러닝 라이브러리와 비교해서 GPU를 이용한 코딩이 매우 쉬움\n",
    "   * 대부분의 참조하는 라이브러리 코드가 Lua에 있으며, 읽기가 쉬움\n",
    "   * 많은 사전 훈련된 모델\n",
    "   \n",
    "과 같은 장점들이 있다. 반면 단점으로는 다음과 같은 단점들이 있는데,\n",
    "   * **Lua**\n",
    "   * Caffe보다 플러그 앤 플레이가 적음\n",
    "   * RNN에는 적합하지 않음\n",
    "   \n",
    "이 있다. Torch는 이전부터 좋은 딥러닝 라이브러리로 평가 받았지만 Lua를 사용한다는 것이 문제가 되었다. <br>\n",
    "물론 Lua를 배워서라도 하려는 사람들이 많을 정도로 좋은 라이브러리지만, 강조했듯이 **Lua**의 진입장벽이 가장 큰 단점으로 작용했고,<br>\n",
    "이 Torch 라이브러리를 Python으로 컨버팅하여 만든것이 바로 PyTorch다.<br>\n",
    "여기에 Pytorch는 Python으로 컨버팅을 하게 되면서 Python의 다른 계산 모듈들을 사용할 수 있게 되는 더욱 큰 장점을 가지게 되었다.\n",
    "~~http://tmmse.xyz/2016/02/25/choosing-deep-learning-libraries/ 참조~~\n",
    "\n",
    "### PyTorch와 Tensorflow의 차이\n",
    "딥러닝 공부를 시작하면서 많은 사람들이 Pytorch와 Tensorflow 둘 중 에서 무엇으로 시작할지 고민을 하고 있고, 가장 많이 사용되는 두가지 딥러닝 라이브러리의 차이를 알기 위해 두 라이브러리의 차이점을 찾아보게 되었다.<br>\n",
    "\n",
    "가장 큰 차이점으로는 Tensorflow는 static하고, Pytorch는 dynamic하다는 것이다.<br>\n",
    "Tensorflow의 경우에는 그래프 구조를 먼저 그려놓고, 그것을 fix해서 데이터가 그 안에서 흐르게 되는 구조로 간단히 요약하면<br>\n",
    "모델을 설정하고 데이터를 돌리는 2가지의 단계로 명확히 구분지어져 있다.<br>\n",
    "해당 구조의 단점은 런타임에서 사용자가 인터렉티브하게 개입하거나 중간에 구조가 바뀌던가 하는 조정이 불가능하다는 것이다.<br><br>\n",
    "\n",
    "반면에 Pytorch의 경우에는 Tensorflow보다 훨씬 늦게 나왔는데, (Torch를 기준으로 1년정도, Pytorch의 경우 2년 정도)<br>\n",
    "나중에 나온 라이브러리인 만큼 Tensorflow의 단점을 수정하고 나오게 되면서 Tensorflow의 단점인 static한 구조를 수정해서 나오게 되었다.<br>\n",
    "\n",
    "또 다른 차이점으로는 Pytorch는 기존에 우리가 하듯이 runtime debugging을 할 수 있지만, Tensorflow는 실행중 중간에 개입하여 디버깅하기가<br>굉장히 힘들다. (tfdbg라는 툴을 이용하면 가능하지만, 처음부터 지원이 되는것과는 비교하기 힘들다.)<br>\n",
    "\n",
    "Tensorflow의 장점으로는 visuallization, 시각화가 있다. Tensorflow의 TensorBoard는 시각화쪽에서 굉장히 강력한 성능을 제공하고 있다.<br>\n",
    "반면에 Pytorch는 공식적인 시각화 툴이 없다. (Visdom이라는 툴이 최근에 제공되기 시작함)<br>\n",
    "\n",
    "또한 Tensorflow의 사용자 커뮤니티가 Pytorch의 사용자 커뮤니티보다 훨씬 크다는 것도 Tensorflow의 큰 장점이다.<br>\n",
    "~~당장 facebook 그룹의 인원부터 Tensorflow KR의 사용자수는 약 4만명인데 비해, Pytorch KR의 사용자는 약 5500명으로 큰차이가 난다.~~<br>\n",
    "그만큼 Tensorflow쪽이 코드를 찾거나, 질문과 답을 찾거나 정보를 구하기가 Pytorch에 비해 훨씬 쉬우며 이로인해 Tensorflow가<br>\n",
    "Pytorch보다 쉽게 느껴지게 된다.<br>\n",
    "~~물론 Pytorch도 기본적인 정보와 커뮤니티는 존재하지만, Tensorflow의 커뮤니티보다 적은건 사실이다.~~<br>\n",
    "\n",
    "Low, High-Level적인 측면에서는 Tensorflow는 점점 라이브러리가 커지면서 거의 C 수준으로 low-Level까지 컨트롤 할 수 있게 컨트롤 할 수 있도록\n",
    "라이브러리가 많이 커졌다.<br>\n",
    "반면 PyTorch는 딥러닝 자체의 구조만을 만들고 실행시키는 것에 관련해서만 집중되어 있다.<br>\n",
    "이로인해 요소, 변수가 많은 실제 업무에서 쓰일 용도로는 Tensorflow가 조금 더 좋고, 공부 또는 개인적인 용도로 사용하는데는 PyTorch가 조금 더 좋다.<br>\n",
    "\n",
    "~~위에 차이점 표 첨부해야될듯~~\n",
    "~~http://bitly.kr/9YDB http://bitly.kr/9YDB 참조~~<br>\n",
    "\n",
    "## Pytorch 시작하기\n",
    "\n",
    "### Tensors\n",
    "\n",
    "#### Tensor는 무엇인가\n",
    "Tensor는 수학과 과학에서 선형 관계를 나타내는 기하적 대상이다. 텐서에 대하여 쉽게 설명을 해보자면,<br>\n",
    "<br>\n",
    "A가 자신의 집으로 가는 방법을 표현한다고 할 경우, A가 \"내 집은 여기서 3km 떨어져 있어\"에서 3km는 스칼라가 되고,<br>\n",
    "\"내 집으로 가려면 여기서 2km를 북쪽으로 간 후 1km 동쪽으로가서 3계단 올라가야 해.\"라는 답으로 3개의 공간 성분을 가진 3차원 벡터가 만들어지는<br> 것을 확인 할 수가 있다.<br>\n",
    "\n",
    "여기서 벡터에 스칼라를 곱하는 경우 v = au 와 같은식으로 쉽게 곱할 수 있다.<br>\n",
    "그런데 방향과 크기를 동시에 바꾸려면 단순히 스칼라를 곱해서는 안된다. 다른 벡터와의 외적 역시 곱하면 무조건 직각으로만 바뀌기 때문에 안된다.<br>\n",
    "따라서 **한번에 방향과 크기를 동시에 바꿔줄 것**이 필요하게 되고, 이를 위해 백터를 복합적으로 연결시킨 구조를 **Tensor**라고 한다.\n",
    "\n",
    "#### Pytorch에서의 Tensor 선언\n",
    "    Pytorch의 Tensor는 NumPy의 ndarray와 유사할뿐만 아니라, GPU를 사용한 연산 가속도 지원한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.00000e-19 *\n",
      "       [[ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  1.6109,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# 초기화 되지 않은 5x3 행렬을 생성한다.\n",
    "x = torch.Tensor(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7750,  0.1933,  0.3376],\n",
      "        [ 0.6186,  0.0879,  0.2701],\n",
      "        [ 0.8082,  0.4008,  0.5986],\n",
      "        [ 0.8133,  0.8259,  0.9688],\n",
      "        [ 0.5040,  0.7888,  0.9928]])\n"
     ]
    }
   ],
   "source": [
    "# 무작위로 초기화된 5x3 행렬을 생성한다.\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "# 행렬의 크기를 구한다.\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor의 연산\n",
    "    PyTorch는 연산을 위한 여러가지 문법을 제공한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7750,  0.1933,  0.3376],\n",
      "        [ 0.6186,  0.0879,  0.2701],\n",
      "        [ 0.8082,  0.4008,  0.5986],\n",
      "        [ 0.8133,  0.8259,  0.9688],\n",
      "        [ 0.5040,  0.7888,  0.9928]])\n",
      "tensor([[ 0.5452,  0.6278,  0.0987],\n",
      "        [ 0.1394,  0.8774,  0.7473],\n",
      "        [ 0.8053,  0.4184,  0.9333],\n",
      "        [ 0.9985,  0.8164,  0.2633],\n",
      "        [ 0.6394,  0.1485,  0.8014]])\n",
      "tensor([[ 1.3202,  0.8211,  0.4363],\n",
      "        [ 0.7580,  0.9653,  1.0174],\n",
      "        [ 1.6134,  0.8192,  1.5319],\n",
      "        [ 1.8118,  1.6423,  1.2321],\n",
      "        [ 1.1434,  0.9373,  1.7941]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(5, 3)\n",
    "# 여러 덧셈 문법\n",
    "print (x)\n",
    "print (y)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2589,  0.6229,  0.9439],\n",
      "        [ 1.3883,  0.2435,  0.7242],\n",
      "        [ 1.4421,  0.9395,  1.4655],\n",
      "        [ 0.9990,  1.6243,  1.9219],\n",
      "        [ 1.4742,  1.0063,  1.1258]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.add(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2589,  0.6229,  0.9439],\n",
      "        [ 1.3883,  0.2435,  0.7242],\n",
      "        [ 1.4421,  0.9395,  1.4655],\n",
      "        [ 0.9990,  1.6243,  1.9219],\n",
      "        [ 1.4742,  1.0063,  1.1258]])\n"
     ]
    }
   ],
   "source": [
    "# 덧셈 결과를 텐서 인자로 제공\n",
    "result = torch.Tensor(5, 3)\n",
    "torch.add(x, y, out=result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2589,  0.6229,  0.9439],\n",
      "        [ 1.3883,  0.2435,  0.7242],\n",
      "        [ 1.4421,  0.9395,  1.4655],\n",
      "        [ 0.9990,  1.6243,  1.9219],\n",
      "        [ 1.4742,  1.0063,  1.1258]])\n"
     ]
    }
   ],
   "source": [
    "# y에 x 더하기\n",
    "y.add_(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1933,  0.0879,  0.4008,  0.8259,  0.7888])\n"
     ]
    }
   ],
   "source": [
    "# Numpy의 인덱싱 표기방법을 사용 할 수도 있음\n",
    "print(x[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "# 텐서의 크기(size)나 모양(shape)을 변경하고 싶을 때, torch.view 를 사용한다.\n",
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)  # 사이즈가 -1인 경우 다른 차원들을 사용하여 유추합니다.\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy 변환(Bridge)\n",
    "    Torch Tensor를 NumPy 배열(array)로 변환하거나, 그 반대로 변하게 하는 것은 매우 쉽다.\n",
    "    Torch Tensor와 NumPy 배열은 저장 공간을 공유하기 때문에, 하나를 변경하면 다른 하나도 변경된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  1.,  1.,  1.,  1.])\n",
      "\n",
      "\n",
      "[1. 1. 1. 1. 1.]\n",
      "\n",
      "\n",
      "tensor([ 2.,  2.,  2.,  2.,  2.])\n",
      "[2. 2. 2. 2. 2.]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Torch Tensor 선언\n",
    "a = torch.ones(5)\n",
    "print(a)\n",
    "print(\"\\n\")\n",
    "# Numpy array 선언\n",
    "b = a.numpy()\n",
    "print(b)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Numpy array와 Torch Tensor사이의 연산\n",
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)\n",
    "print(\"\\n\") # 한쪽에만 더해주었는데 같이 변하는 걸 확인 - 저장공간을 공유함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([ 2.,  2.,  2.,  2.,  2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 반대로 np배열의 값을 변화시켜도 torch tensor의 값도 변한다.\n",
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA Tensors\n",
    "    .cuda 메소드를 사용하여 Tensor를 GPU 상으로 옮길 수 있다.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (16) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-d0d8a255c6d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (16) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# 이 코드는 CUDA가 사용 가능한 환경에서만 실행합니다.\n",
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
